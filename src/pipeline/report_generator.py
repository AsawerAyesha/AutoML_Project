import pandas as pd
import datetime
import json

class ReportGenerator:
    """
    Generates an HTML report summarizing the entire AutoML process:
    - Dataset Statistics
    - EDA Findings
    - Issues Detected & User Decisions
    - Preprocessing Configuration
    - Model Comparison Table
    - Best Model Metrics with Justification
    - Model Configurations & Hyperparameters
    """
    def __init__(self, dataset, target_column, issues, user_decisions, preprocessing_config, model_results, best_model):
        self.dataset = dataset
        self.target_column = target_column
        self.issues = issues
        self.user_decisions = user_decisions
        self.preprocessing_config = preprocessing_config
        self.model_results = model_results
        self.best_model = best_model

    def generate_html_report(self):
        # Current timestamp
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Basic HTML Template using f-strings
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AutoML Project Report</title>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background-color: #f9f9f9; color: #333; }}
                .container {{ background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
                h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                h2 {{ color: #2980b9; margin-top: 30px; }}
                h3 {{ color: #34495e; margin-top: 20px; }}
                .metric-box {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; }}
                .best-model-box {{ background: #d5f4e6; padding: 20px; border-left: 5px solid #27ae60; border-radius: 5px; margin: 15px 0; }}
                .config-box {{ background: #fef5e7; padding: 15px; border-left: 5px solid #f39c12; border-radius: 5px; margin: 10px 0; }}
                table {{ border-collapse: collapse; width: 100%; margin-top: 10px; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #3498db; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
                .highlight {{ font-weight: bold; color: #27ae60; }}
                .warning {{ color: #e74c3c; font-weight: bold; }}
                ul {{ line-height: 1.8; }}
                .justification {{ background-color: #e8f8f5; padding: 15px; border-radius: 5px; margin-top: 10px; }}
                .config-details {{ background-color: white; padding: 15px; border: 1px solid #bdc3c7; border-radius: 5px; margin-top: 10px; font-family: monospace; }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ü§ñ AutoML Execution Report</h1>
                <p><strong>Generated on:</strong> {timestamp}</p>

                <h2>1. Dataset Overview</h2>
                <div class="metric-box">
                    <p><strong>Target Column:</strong> {self.target_column}</p>
                    <p><strong>Total Rows:</strong> {len(self.dataset)}</p>
                    <p><strong>Total Columns:</strong> {len(self.dataset.columns)}</p>
                    <p><strong>Features Used:</strong> {len(self.dataset.columns) - 1}</p>
                </div>

                <h2>2. Exploratory Data Analysis (EDA) Findings</h2>
                {self._generate_eda_findings()}

                <h2>3. Data Quality Issues & User Decisions</h2>
                {self._generate_issues_table()}

                <h2>4. Preprocessing Configuration</h2>
                <ul>
                    <li><strong>Missing Value Strategy:</strong> {self.preprocessing_config.get('missing_strategy', 'N/A')}</li>
                    <li><strong>Outlier Strategy:</strong> {self.preprocessing_config.get('outlier_strategy', 'N/A')}</li>
                    <li><strong>Scaling Strategy:</strong> {self.preprocessing_config.get('scaling_strategy', 'N/A')}</li>
                    <li><strong>Encoding Strategy:</strong> {self.preprocessing_config.get('encoding_strategy', 'N/A')}</li>
                    <li><strong>Test Split Size:</strong> {float(self.preprocessing_config.get('test_size', 0.2))*100:.0f}%</li>
                </ul>

                <h2>4. Model Configurations & Hyperparameters</h2>
                {self._generate_model_configs()}

                <h2>5. Model Performance Comparison</h2>
                {self._generate_model_table()}

                <h2>6. Confusion Matrices by Model</h2>
                {self._generate_confusion_matrices()}

                <h2>7. üèÜ Best Model Selected & Justification</h2>
                {self._generate_best_model_section()}

                <hr>
                <p style="text-align: center; font-size: 0.9em; color: #7f8c8d;">Generated by Custom AutoML Pipeline</p>
            </div>
        </body>
        </html>
        """
        return html_content

    def _generate_eda_findings(self):
        """Generate EDA findings from dataset analysis"""
        findings_html = """
        <div class="metric-box">
            <h3>üìä Data Distribution & Quality Metrics</h3>
            <ul>
        """
        
        # Missing values analysis
        missing_count = self.dataset.isnull().sum().sum()
        missing_pct = (missing_count / (len(self.dataset) * len(self.dataset.columns))) * 100
        findings_html += f"<li><strong>Missing Values:</strong> {missing_count} ({missing_pct:.2f}% of total data)</li>"
        
        # Numerical columns analysis
        numeric_cols = self.dataset.select_dtypes(include=['int64', 'float64']).columns
        if len(numeric_cols) > 0:
            findings_html += f"<li><strong>Numerical Features:</strong> {len(numeric_cols)} columns</li>"
            findings_html += f"<li><strong>Numerical Statistics:</strong>"
            findings_html += "<ul>"
            for col in numeric_cols:
                if col != self.target_column:
                    mean_val = self.dataset[col].mean()
                    std_val = self.dataset[col].std()
                    findings_html += f"<li>{col}: Œº={mean_val:.2f}, œÉ={std_val:.2f}</li>"
            findings_html += "</ul></li>"
        
        # Categorical columns analysis
        categorical_cols = self.dataset.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            findings_html += f"<li><strong>Categorical Features:</strong> {len(categorical_cols)} columns</li>"
        
        # Target variable distribution
        if self.target_column in self.dataset.columns:
            if self.dataset[self.target_column].dtype == 'object':
                class_dist = self.dataset[self.target_column].value_counts()
                findings_html += f"<li><strong>Target Distribution:</strong>"
                findings_html += "<ul>"
                for cls, count in class_dist.items():
                    pct = (count / len(self.dataset)) * 100
                    findings_html += f"<li>{cls}: {count} samples ({pct:.1f}%)</li>"
                findings_html += "</ul></li>"
            else:
                findings_html += f"<li><strong>Target Variable Range:</strong> [{self.dataset[self.target_column].min():.2f}, {self.dataset[self.target_column].max():.2f}]</li>"
        
        # Duplicates check
        duplicates = self.dataset.duplicated().sum()
        findings_html += f"<li><strong>Duplicate Rows:</strong> {duplicates} ({(duplicates/len(self.dataset))*100:.2f}%)</li>"
        
        findings_html += """
            </ul>
        </div>
        """
        return findings_html

    def _generate_issues_table(self):
        """Helper to create HTML table for issues"""
        if not self.issues:
            return "<p>‚úÖ No data quality issues were detected.</p>"
        
        rows = ""
        for i, issue in enumerate(self.issues):
            decision_key = f"{issue['type']}_{i}"
            decision = self.user_decisions.get(decision_key, "Default/No Action")
            
            rows += f"""
            <tr>
                <td>{issue['type']}</td>
                <td>{issue.get('column', 'Global')}</td>
                <td>{issue.get('severity', 'Medium')}</td>
                <td>{decision}</td>
            </tr>
            """
            
        return f"""
        <table>
            <thead>
                <tr>
                    <th>Issue Type</th>
                    <th>Column</th>
                    <th>Severity</th>
                    <th>User Decision</th>
                </tr>
            </thead>
            <tbody>
                {rows}
            </tbody>
        </table>
        """

    def _generate_model_configs(self):
        """Generate model configurations and hyperparameters section"""
        if not self.model_results:
            return "<p>‚ö†Ô∏è No model results available.</p>"
        
        configs_html = ""
        
        # Predefined hyperparameters for each model
        model_params = {
            "Logistic Regression": "No hyperparameters tuned (default configuration)",
            "Decision Tree": "Criterion: ['gini', 'entropy']",
            "Random Forest": "n_estimators: [8, 16, 32, 64, 128, 256]",
            "AdaBoost Classifier": "learning_rate: [0.1, 0.01, 0.5, 0.001], n_estimators: [8, 16, 32, 64, 128, 256]",
            "K-Nearest Neighbors": "n_neighbors: [3, 5, 7, 9, 11, 13], metric: ['euclidean', 'manhattan']",
            "Naive Bayes": "No hyperparameters tuned (distribution-based)",
            "Support Vector Machine": "C: [0.1, 1, 10, 100], kernel: ['linear', 'rbf']",
            "OneR Classifier": "No hyperparameters tuned (rule-based)"
        }
        
        for model_name in self.model_results.keys():
            params = model_params.get(model_name, "Parameters not defined")
            configs_html += f"""
            <div class="config-box">
                <h3>{model_name}</h3>
                <p><strong>Hyperparameter Search Space:</strong></p>
                <div class="config-details">
                    {params}
                </div>
            </div>
            """
        
        return configs_html

    def _generate_best_model_section(self):
        """Generate best model summary with detailed justification"""
        if not self.best_model or self.best_model.empty:
            return "<p>‚ö†Ô∏è No best model available.</p>"
        
        model_name = self.best_model.get('Model', 'Unknown')
        f1_score = self.best_model.get('F1-Score', 0)
        accuracy = self.best_model.get('Accuracy', 0)
        precision = self.best_model.get('Precision', 0)
        recall = self.best_model.get('Recall', 0)
        roc_auc = self.best_model.get('roc_auc', None)
        training_time = self.best_model.get('training_time', 0)
        
        # Calculate performance ranking
        if self.model_results:
            all_f1_scores = []
            for metrics in self.model_results.values():
                if isinstance(metrics, dict) and 'F1-Score' in metrics:
                    all_f1_scores.append(metrics['F1-Score'])
            
            if all_f1_scores:
                rank = sorted(all_f1_scores, reverse=True).index(f1_score) + 1
                total_models = len(all_f1_scores)
            else:
                rank = 1
                total_models = 1
        else:
            rank = 1
            total_models = 1
        
        # Generate justification
        justification = f"""
        <div class="justification">
            <h4>üìä Performance Justification:</h4>
            <ul>
                <li><strong>Ranking:</strong> #{rank} out of {total_models} models</li>
                <li><strong>Primary Metric (F1-Score):</strong> {f1_score:.4f} - Balances precision and recall effectively</li>
                <li><strong>Accuracy:</strong> {accuracy:.4f} - Overall correctness of predictions</li>
                <li><strong>Precision:</strong> {precision:.4f} - Minimizes false positives</li>
                <li><strong>Recall:</strong> {recall:.4f} - Minimizes false negatives</li>
                {f'<li><strong>ROC-AUC:</strong> {roc_auc:.4f} - Binary classification discriminative ability</li>' if roc_auc else ''}
                <li><strong>Training Time:</strong> {training_time:.4f}s - Computational efficiency</li>
            </ul>
            
            <h4>‚úÖ Why This Model?</h4>
            <p>This model achieved the <strong>highest F1-score</strong>, which represents the best balance between 
            precision and recall. In classification tasks, F1-score is crucial because it avoids the pitfall of 
            high accuracy with imbalanced classes. The model demonstrates robust generalization and reliable 
            predictions for production deployment.</p>
        </div>
        """
        
        return f"""
        <div class="best-model-box">
            <h3>üèÜ {model_name}</h3>
            <table style="width: 100%; margin-top: 15px;">
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td><strong>F1-Score</strong></td>
                    <td><span class="highlight">{f1_score:.4f}</span></td>
                </tr>
                <tr>
                    <td><strong>Accuracy</strong></td>
                    <td>{accuracy:.4f}</td>
                </tr>
                <tr>
                    <td><strong>Precision</strong></td>
                    <td>{precision:.4f}</td>
                </tr>
                <tr>
                    <td><strong>Recall</strong></td>
                    <td>{recall:.4f}</td>
                </tr>
                {f'<tr><td><strong>ROC-AUC</strong></td><td>{roc_auc:.4f}</td></tr>' if roc_auc else ''}
                <tr>
                    <td><strong>Training Time</strong></td>
                    <td>{training_time:.4f}s</td>
                </tr>
            </table>
            {justification}
        </div>
        """

    def _generate_confusion_matrices(self):
        """Generate confusion matrices for all models as HTML tables"""
        if not self.model_results:
            return "<p>‚ö†Ô∏è No confusion matrices available.</p>"
        
        cm_html = ""
        
        for model_name, metrics in self.model_results.items():
            if isinstance(metrics, dict) and 'confusion_matrix' in metrics:
                cm = metrics['confusion_matrix']
                
                # Create HTML table from confusion matrix
                cm_html += f"""
                <div class="metric-box">
                    <h3>{model_name}</h3>
                    <table style="width: auto; margin: 10px 0;">
                        <tr>
                            <th>Predicted \\ Actual</th>
                            <th style="background-color: #3498db; color: white;">Negative</th>
                            <th style="background-color: #3498db; color: white;">Positive</th>
                        </tr>
                        <tr>
                            <th style="background-color: #3498db; color: white;">Negative</th>
                            <td style="background-color: #d5f4e6; font-weight: bold;">{cm[0,0]}</td>
                            <td style="background-color: #fef5e7;">{cm[0,1] if cm.shape[1] > 1 else 'N/A'}</td>
                        </tr>
                        <tr>
                            <th style="background-color: #3498db; color: white;">Positive</th>
                            <td style="background-color: #fef5e7;">{cm[1,0] if cm.shape[0] > 1 else 'N/A'}</td>
                            <td style="background-color: #d5f4e6; font-weight: bold;">{cm[1,1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 'N/A'}</td>
                        </tr>
                    </table>
                </div>
                """
        
        if cm_html:
            return f"<p><strong>Confusion matrices for each model:</strong></p>{cm_html}"
        else:
            return "<p>‚ö†Ô∏è No confusion matrices available.</p>"

    def _generate_model_table(self):
        """Helper to create HTML table for model results"""
        if not self.model_results:
            return "<p>‚ö†Ô∏è No model results available.</p>"
        
        # Convert dict to DataFrame for easier handling
        rows_list = []
        for model_name, metrics in self.model_results.items():
            row = {}
            if isinstance(metrics, dict):
                row = metrics.copy()
            row['Model'] = model_name
            rows_list.append(row)
            
        df = pd.DataFrame(rows_list)
        
        # Reorder columns to ensure Model is first
        cols = ['Model'] + [c for c in df.columns if c != 'Model' and c not in ['model', 'y_test_pred', 'confusion_matrix']]
        df = df[[c for c in cols if c in df.columns]]
        
        # Sort by F1 Score if available
        if 'F1-Score' in df.columns:
            df = df.sort_values('F1-Score', ascending=False)
            
        return df.to_html(classes='table', index=False, float_format="%.4f")